<html>
<head><title>Prime Factorization in CUDA</title></head>
<body>
<h1>Factoring Large Numbers by Parallelizing Dixon's Method</h1>
<h4>Benjamin Zagorsky, Loren McGinnis</h4>
<h3>Motivation</h3>

<p>
The best known algorithms for factoring numbers all require super-polynomial
 time.  Cryptographic methods, such as RSA, rely upon this fact.  Thus, prime factorization of very large numbers is an important and interesting concept to explore.  Algorithms for factorization often have components that are embarrassingly parallel.  Our goal is to take advantage of this by porting a particular factorization algorithm, <a href=”http://en.wikipedia.org/wiki/Dixon's_factorization_method”>Dixon's method</a>, to CUDA.
</p>
<h3>
Dixon's in a nutshell
</h3>
<p>
The above link has more information about how Dixon's algorithm works, but for our purposes, there are two major points: B-smooth numbers, and congruence of squares.  The latter states that given a number N, if we can find two numbers x and y such that their squares are congruent mod N, (x+y) and (x-y) will each contain a factor of N.  These numbers are rare and hard to find efficiently with a naive search.  Dixon's method utilizes B-smooth numbers to aid in finding congruent squares, leading to factors of N.
</p>
<p>
For a number A to be "B-smooth," all primes that divide A must be smaller than B. For example, 10-smooth numbers, when decomposed into primes, can be represented as 2^a*3^b*5^c*7^d.  B-smooth numbers are important because given a small quantity of numbers whose square mod N are B-smooth, a congruence of squares can be generated by multiplying some subset of them together.  This is the premise of Dixon's method. The power of it is that small is only a few more than the number of primes less than or equal to B. All that remains is then finding B-smooth numbers and then generating a congruence of squares from them. For fixed B, the post-processing of the B-smooth numbers can be done in constant time.  Thus, all that really needs optimizing is finding the B-smooth numbers themselves. Better yet, testing numbers for B-smoothness can be done independently, so this component of the algorithm can be parallelized.
</p>
<h3>
 B-smooth numbers in CUDA
</h3>
<p>
The bulk of our code is a CUDA kernel that searches for numbers A between the square root of N and N that satisfy the following condition:  A*A mod N is B-smooth. With enough of these numbers (and we only need a few, as mentioned above), some combination of their product will result in a square and possibly yield a factor.  The kernel searches for one B-smooth number per block.   The distribution of B-smooth numbers is uniform overall, so we simply begin checking the first T numbers greater than the square root of N, then jumping to the next T numbers, and so on.  When a block finds a B-smooth number, it writes out the number to global memory and exits.
</p>
<h3>
PyCUDA: Big numbers, meta-programming, and auto-tuning
</h3>
<p>
We have included in our source code a simple implementation written in pure C++/CUDA, where the B-smooth kernel is hard-coded.  However, there were some downsides to doing this.  The most obvious is the maximum size data-types that CUDA supports.  Since there is a square operation in our algorithm, the largest N that CUDA can operate on with 64-bit integer types is smaller than 32 bits.  Doing factorization on numbers this small is not very interesting, since a serial, naive approach can factor numbers of this size in little time on modern hardware.
</p>
<p>
Thus, our next milestone was to build support into CUDA, in the form of in-line subroutines, that would allow for arbitrarily large numbers.  To hard-code this would be difficult, since we want to limit the amount of memory overhead in storing large numbers.  PyCUDA became and ideal fit for this, since we could template the subroutines we needed for operations on large numbers with python, and dynamically decide the sizes of these numbers at runtime.  Thus, the B-smooth search runs in a runtime-generated CUDA kernel, and the post-processing that extracts factors runs in Python.
</p>
<p>
PyCUDA also allowed us to tune other important variables in our algorithm, such as the value of B.  Having a higher value of B increases the density of B-smooth numbers in the search space, but also makes the calculations more intensive.  Thus, while we may find B-smooth numbers faster, extracting factors from the may take longer.
</p>

<h3>
Usage
</h3>
<p>
The interface to our program is quite simple.  The version that supports arbitrarily large numbers is dixons_pycuda.py.  When running a gpu instance on resonance, and assuming the modules for PyCUDA are loaded, calculating factors for a prime number N can be done simply by running:
</p>
<pre>
python dixons_pycuda.py N [B]
</pre>
<p>
Where B is an optional parameter indicating that we want to search for numbers of smoothness B.  As documentation of our progress, we also include a plain (no parallelization) implementation of Dixon's, dixons.py, and a hard-coded CUDA/C++ implementation that also calculates B-smooth numbers, but can only work on inputs of limited size.
</p>
<h3>
Evaluation
</h3>

<p>
By auto-tuning the value of B, we were able to find the most efficient factor base for both discovering B-smooth numbers and extracting factors from them.  Preliminary results indicate that we achieved a speedup of over 800x against a regular implementation of Dixon's.  Unfortunately, however, our improvements are still limited by the asymptotic behavior of factorization, and runtime increases sharply as the the bit-size of our input increases.
</p><p>
This could be mitigated somewhat with more computing power.  Since much of our efforts went into supporting big numbers on CUDA, we did not have the time to extend our implementation to a clustered environment.  However, our code easily lends itself to such an extension, as the blocks in a kernel run almost entirely independently of each other.  Just changing a parameter to the function that generates the CUDA kernel is sufficient to make the kernel work on multiple nodes without causing unnecessary overlap of work.  In addition, the second portion of the algorithm, extracting factors from the B-smooth numbers, also have portions that could be parallelized.  In our experience, however, since we have had success with small values of B on a variety of input sizes, the amount of work this portion of the algorithm has to do essentially remains constant.
</p>

<h3>
Afterthoughts
</h3>

<p>
When preparing for this project, we did not correctly anticipate where the bulk of the work would be.  This became implementing big numbers in CUDA.  While in and of itself this has the potential to be useful, it took away from other features we wanted to support, such as clustering.  Unfortunately, the problem only becomes interesting when dealing which really large numbers, so big number support was vital to the efficacy of our project.  If we had more time to work on our project, the next steps would definitely be to extend this to a cluster environment.
</p>
</body>
</html>
